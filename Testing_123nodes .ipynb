{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os, scipy\n",
    "import torch   \n",
    "import scipy.sparse as sp \n",
    "import numpy as np  \n",
    "import torch.nn.functional as F \n",
    "import pickle\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt \n",
    "from math import *\n",
    "from numpy import transpose,matrix,exp,conj\n",
    "from numpy.linalg import inv \n",
    "from util_final import * \n",
    "import torch.nn as nn\n",
    "from load_data import *\n",
    "from PPGN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters  \n",
    "'''\n",
    "faulttype: the type of the fault: 'pp' is phase to phase fault; 'sp3' is single phase fault; 'ppg' is phase to phase to ground; 'all' is the combination of all these three faults.\n",
    "num_labelper: the number of labeled data for each label (<120). If test the trained models, please choose 20,33,67,100 for ppg and sp3, choose 4,5,7,10\n",
    "dropout: whether has dropout operation\n",
    "root: the root path\n",
    "num_node: the number of the graph in stage I\n",
    "dim_input: the dimension of the input at each node\n",
    "agg_func: the way of aggregate the hidden variables of its neighbors is to calculate mean\n",
    "seed: random seed\n",
    "hidden_emb_size: the size of the hidden variables in each layer\n",
    "num_layers: the number of layers\n",
    "dataSet: the name of the dataset\n",
    "device: cpu\n",
    "lr: learning rate\n",
    "weight_name: choose the adjacency matrix: 'A_short' is the proposed using shortest path; 'A_adam' is the normalized admittance matrix\n",
    "k: denotes k_I, the nearest k neighbors are used to learn the hidden variables\n",
    "measured_index: the measured nodes in power grids\n",
    "A: the index of the neighbors of each node\n",
    "prob_A: The adjacency matrix A\n",
    "modelname: the name of the saved model\n",
    "savepath: the root path of saved model \n",
    "'''\n",
    "faulttype = 'ppg'\n",
    "num_labelper = 100\n",
    "dropout = 0 \n",
    "root = \"./data/\" \n",
    "num_node = 128   \n",
    "dim_input = 6\n",
    "agg_func = \"MEAN\" \n",
    "global seed\n",
    "seed = 842  \n",
    "hidden_emb_size = [32,32,32] \n",
    "num_layers = len(hidden_emb_size) \n",
    "dataSet = 'loc' \n",
    "device = \"cpu\"  \n",
    "lr = 0.001\n",
    "weight_name = 'A_short' \n",
    "k=3\n",
    "measured_index =[73, 94, 105, 118, 72, 79, 24, 41, 69, 90, 84, 78, 122, 49, 66, 104, 109, 10, 36, 31, 85]\n",
    "A, prob_A = select_A_prob(k, 'A_short')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data   \n",
    "name_train = 'train_set_allV.npz'\n",
    "name_test = 'test_set_allV.npz'\n",
    "if faulttype == 'all':\n",
    "    features, labels, ind_train ,  ind_test,    neib_observ,ind_labels, ind_measured= load_all_types(name_train, name_test, num_labelper , measured_index,seed = seed )\n",
    "else:\n",
    "    features, labels, ind_train ,  ind_test,    neib_observ,ind_labels, ind_measured= load_data_single_observ( name_train, name_test, num_labelper ,seed, measured_index,  phase = faulttype )\n",
    "with open(os.path.join(root, 'neib.pickle'), 'rb') as f:\n",
    "    dic = pickle.load(f)\n",
    "neib  = dic['neib']\n",
    "random.seed( seed)\n",
    "np.random.seed( seed)\n",
    "torch.manual_seed( seed)\n",
    "torch.cuda.manual_seed_all( seed)\n",
    "dataC = dataCenter(name_train, name_test,num_labelper,seed, measured_index, faulttype) \n",
    "nodes_layers  = dic_nodes_neib(num_layers,A,prob_A ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_stageI = GraphSage(num_layers, dim_input,hidden_emb_size  ,   A, prob_A, dropout = dropout ,device = device,    agg_method= agg_func  )\n",
    "graph_stageI.to(device) \n",
    "classification = Outlayer_fully( hidden_emb_size[-1],  num_node,  dropout = dropout ) \n",
    "classification.to(device)\n",
    "models = [graph_stageI, classification]\n",
    "params = []\n",
    "for model in models:\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            params.append(param)\n",
    "optimizer = torch.optim.Adam(params, lr = lr, weight_decay = 5e-3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './trained/sup_100_ppg_21.pkl'\n",
      "=> loaded checkpoint './trained/sup_100_ppg_21.pkl' (epoch 287)\n",
      "Test F1:0.9761,  Acc:0.9845, Acc 1 hop: 0.9864 \n"
     ]
    }
   ],
   "source": [
    "# Test the trained model \n",
    "modelname =  'sup_' +  str(num_labelper) + '_' + faulttype +  '_21.pkl'\n",
    "saveroot = \"./trained\"\n",
    "#'./00_saved_final'\n",
    "savebest = os.path.join(saveroot, modelname ) \n",
    "models, optimizer, start_epoch = load_checkpoint(models, optimizer , savebest)\n",
    "graph_stageI, classification = models[0], models[1]\n",
    "labels_neib = one_hot_neib(labels, neib )\n",
    "models = [graph_stageI, classification]\n",
    "test_acc, test_acc_neib, test_f1 , embs, logists=  test1( graph_stageI, classification,nodes_layers, features, ind_test, labels_neib, labels)  \n",
    "print(\"Test F1:%.4f,  Acc:%.4f, Acc 1 hop: %.4f \" %(test_f1, test_acc, test_acc_neib))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
